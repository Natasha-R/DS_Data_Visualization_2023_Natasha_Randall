{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a86b0315-9f20-4001-a1e1-8b82338a6c36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import javalang\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04898a30-52b0-44a0-858a-e94d8d678a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"processed_code_solutions/all_code_data.csv\")\n",
    "file_data = pd.read_csv(\"processed_code_solutions/files_code_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf54ad-76f8-40c0-8339-37e94a107a9e",
   "metadata": {},
   "source": [
    "Feature engineering directly from the code and from the AST tree representation of the code via javalang:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e41fc859-bb43-40ec-9694-abf882824721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features(data, file_data):\n",
    "    data = data.copy()\n",
    "    file_data = file_data.copy()\n",
    "\n",
    "    data[\"num_chars\"] = data[\"code\"].str.len()\n",
    "    data[\"num_lines\"] = data[\"code\"].apply(lambda row: len(row.splitlines()))\n",
    "    data[\"avg_line_length\"] = data[\"code\"].apply(lambda row: np.mean([len(line) for line in row.splitlines() if len(line)>0]))\n",
    "    data[\"max_line_length\"] = data[\"code\"].apply(lambda row: max([len(line) for line in row.splitlines() if len(line)>0]))\n",
    "    data[\"num_scaled_comments\"] = (data[\"code\"].str.count(\"//\") + data[\"code\"].str.count(\"/\\*\")) / data[\"num_lines\"]\n",
    "    data[\"num_scaled_if\"] = data[\"code\"].str.count(\"if\") / data[\"num_lines\"]\n",
    "    data[\"num_scaled_for\"] = data[\"code\"].str.count(\"for\") / data[\"num_lines\"]\n",
    "    data[\"num_scaled_switch\"] = data[\"code\"].str.count(\"switch\") / data[\"num_lines\"]\n",
    "    data[\"num_scaled_digits\"] = (data[\"code\"].apply(lambda row: len([value for value in row if value.isdigit()]))) / data[\"num_lines\"]\n",
    "    data[\"num_scaled_exceptions\"] = data[\"code\"].str.count(\"throw new\") / data[\"num_lines\"]\n",
    "    data[\"num_scaled_empty_lines\"] = data[\"code\"].str.count(\"\\n\\n\") / data[\"num_lines\"]\n",
    "    data[\"num_scaled_prints\"] = data[\"code\"].str.count(\"print\") / data[\"num_lines\"]\n",
    "\n",
    "    file_data[\"num_method_declarations\"] = file_data[\"code\"].apply(lambda row: len([node.name for path, node in javalang.parse.parse(row).filter(javalang.tree.MethodDeclaration)]))\n",
    "    file_data[\"num_declared_fields\"] = file_data[\"code\"].apply(lambda row: len([declarator.name for path, node in javalang.parse.parse(row).filter(javalang.tree.FieldDeclaration) for declarator in node.declarators]))\n",
    "    file_data[\"num_local_variables\"] = file_data[\"code\"].apply(lambda row: len([declarator.name for path, node in javalang.parse.parse(row).filter(javalang.tree.LocalVariableDeclaration) for declarator in node.declarators]))\n",
    "    file_data[\"num_classes\"] = file_data[\"code\"].apply(lambda row: len([node.name for path, node in javalang.parse.parse(row).filter(javalang.tree.ClassDeclaration)]))\n",
    "    file_data[\"num_field_references\"] = file_data[\"code\"].apply(lambda row: len([node.member for path, node in javalang.parse.parse(row).filter(javalang.tree.MemberReference)]))\n",
    "    file_data[\"num_method_invocations\"] = file_data[\"code\"].apply(lambda row: len([node.member for path, node in javalang.parse.parse(row).filter(javalang.tree.MethodInvocation)]))\n",
    "    file_data[\"num_imports\"] = file_data[\"code\"].apply(lambda row: len([node.path for path, node in javalang.parse.parse(row).filter(javalang.tree.Import)]))\n",
    "    file_data[\"used_field_types\"] = (file_data[\"code\"].apply(lambda row: [node.type.name for path, node in javalang.parse.parse(row).filter(javalang.tree.FieldDeclaration)]) \n",
    "                                     + file_data[\"code\"].apply(lambda row: [node.type.name for path, node in javalang.parse.parse(row).filter(javalang.tree.LocalVariableDeclaration)])).apply(set)\n",
    "    for data_type in [\"boolean\", \"List\", \"Integer\", \"Point\", \"ArrayList\", \"StringBuilder\"]:\n",
    "        file_data[f\"used_{data_type}\"] = file_data[\"used_field_types\"].apply(lambda row: data_type in row)\n",
    "    file_data[\"variable_names\"] = file_data[\"code\"].apply(lambda row: list({node.member for path, node in javalang.parse.parse(row).filter(javalang.tree.MemberReference)}))\n",
    "    \n",
    "    variable_names = file_data.groupby([\"name\", \"source\", \"style\", \"version\"], dropna=False)[\"variable_names\"].sum().reset_index()\n",
    "    variable_names[\"avg_name_length\"] = variable_names[\"variable_names\"].apply(lambda row: np.mean([len(value) for value in row]))\n",
    "    variable_names[\"max_name_length\"] = variable_names[\"variable_names\"].apply(lambda row: np.max([len(value) for value in row]))\n",
    "    variable_names.drop(\"variable_names\", axis=1, inplace=True)\n",
    "    file_data.drop(\"variable_names\", axis=1, inplace=True)\n",
    "    file_data[\"all_comments\"] = file_data[\"code\"].apply(lambda row: [comment for comment in [line.strip() for comment in re.findall('/\\*.*?\\*/', row, re.DOTALL) for line in comment.splitlines()] if comment != \"/*\" and comment != \"*/\"]\\\n",
    "                                                      + [comment[2:].strip() for comment in re.findall('//.*', row)])\n",
    "    variable_names = variable_names.merge(file_data.groupby([\"name\", \"source\", \"style\", \"version\"], dropna=False)[\"all_comments\"].sum().reset_index(), on = [\"name\", \"source\", \"style\", \"version\"])\n",
    "    variable_names[\"avg_comment_length\"] = variable_names[\"all_comments\"].apply(lambda row: 0 if len(row) == 0 else np.mean([len(value) for value in row]))\n",
    "    variable_names[\"max_comment_length\"] = variable_names[\"all_comments\"].apply(lambda row: 0 if len(row) == 0 else np.max([len(value) for value in row]))\n",
    "    variable_names.drop(\"all_comments\", axis=1, inplace=True)\n",
    "    file_data.drop(\"all_comments\", axis=1, inplace=True)\n",
    "\n",
    "    file_data[\"num_files\"] = 1\n",
    "\n",
    "    grouped_file_data = file_data.groupby([\"name\", \"source\", \"style\", \"version\"], dropna=False).sum().reset_index()\n",
    "    for data_type in [\"boolean\", \"List\", \"Integer\", \"Point\", \"ArrayList\", \"StringBuilder\"]:\n",
    "        grouped_file_data[f\"used_{data_type}\"] = grouped_file_data[f\"used_{data_type}\"].astype(bool)\n",
    "    \n",
    "    grouped_file_data = grouped_file_data.merge(data[[\"name\", \"source\", \"style\", \"version\", \"num_lines\"]], on=[\"name\", \"source\", \"style\", \"version\"])\n",
    "    grouped_file_data_columns = [column for column in grouped_file_data.columns if column[:3]==\"num\" and column != \"num_files\"]\n",
    "\n",
    "    for column in grouped_file_data_columns:\n",
    "        grouped_file_data[f\"num_scaled_{column[4:]}\"] = grouped_file_data[column] / grouped_file_data[\"num_lines\"]\n",
    "        grouped_file_data.drop(column, axis=1, inplace=True)\n",
    "\n",
    "    data = data.merge(grouped_file_data.drop(\"num_scaled_lines\", axis=1), on=[\"name\", \"source\", \"style\", \"version\"])\n",
    "    data = data.merge(variable_names, on=[\"name\", \"source\", \"style\", \"version\"])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c16e5c-4aaa-4185-9dd4-8377d3487edd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_data = extract_features(data, file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484bf4d9-63c7-4d72-864e-956e206160dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db4f7f2-5cbd-497a-9ce0-d10a2fc86222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(features_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92edc53-47c7-4b03-94ac-951e69136b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_data.to_csv(\"processed_code_solutions/features_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fb44f8-51dc-4b89-bd43-7939867f0c37",
   "metadata": {},
   "source": [
    "If the features are not scaled, then they simply reflect the length of the code (see correlation matrix below) - e.g. longer code naturally has more field references, etc. \n",
    "By dividing each feature by the length of the code, we can extract whether a relatively large or small number of e.g. field references have been included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b572d161-72bb-4e94-9c68-8b76fd9e0609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_scaled_data = features_data.copy()\n",
    "scaled_columns = [column for column in features_data.columns if \"scaled\" in column]\n",
    "for column in scaled_columns:\n",
    "    non_scaled_data[column] = non_scaled_data[column] * non_scaled_data[\"num_lines\"]\n",
    "\n",
    "non_scaled_data.columns = list(pd.Series(non_scaled_data.columns).str.replace(\"_scaled\", \"\"))\n",
    "plt.subplots(figsize=(22, 10))\n",
    "sns.heatmap(non_scaled_data.corr(), center=0, annot=True, mask=np.triu(np.ones_like(features_data.corr(), dtype=bool)));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
